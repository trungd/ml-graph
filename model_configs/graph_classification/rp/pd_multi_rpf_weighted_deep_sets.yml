backend: pytorch
random_seed: 123
env:
  default:
    variables:
      batch_size: 256
      dropout: 0.0
      learning_rate: 0.01
      early_stop: 50
      encoder_dim: 64
      decoder_dim: 64
      combine_encoder: pooling
      num_epochs: 500
      multi_encoder: false
      quantize_type: none
      quantize_size: 100
      append_embedding_to_encoder_output: true
      shared_encoder_dim: 64, 64
  mutag:
    default: false
    variables:
      dataset: MUTAG
      feature_name:
        - multi_1.2.3.4
        - multi_1.2.3.4.5.6.7.8
      signature: [rpf]
      hidden_dim: [16, 32]
      multi_encoder: true
      num_epochs: 20
      num_encoder_layers: 1
      num_decoder_layers: 1
  mutag_attn:
    default: false
    variables:
      dataset: MUTAG
      feature_name:
        - multi_1.2.3.4
      signature: [rpf]
      num_epochs: 200
      num_encoder_layers: 1
      num_decoder_layers: 1
      combine_encoder: attention
  nci:
    default: false
    variables:
      dataset:
        - NCI1
        #- NCI109
      feature_name:
        - freq_multi_1.2.3.4.5.6.7.8
        - freq_multi_1.2.3.4
        - freq_multi_1.3.5.7
        #- freq_multi_1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16
        #- freq_multi_1.3.5.7.9.11.13.15
      encoder_dim: 64
      shared_encoder_dim: 64, 128
      decoder_dim: 128, 64
      hidden_dim: 128
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      batch_size: 256
      num_epochs: 200
      multi_encoder: true
  nci_attn:
    default: false
    variables:
      dataset:
        - NCI1
        #- NCI109
      feature_name:
        - freq_multi_1.2.3.4.5.6.7.8
        #- freq_multi_1.2.3.4
        #- freq_multi_1.3.5.7
        #- freq_multi_1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16
        #- freq_multi_1.3.5.7.9.11.13.15
      encoder_dim: 64, 128
      decoder_dim: 128, 64, 32, 16
      hidden_dim: 128
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      batch_size: 256
      num_epochs: 200
      multi_encoder: true
      combine_encoder: attention
  dd:
    default: false
    variables:
      dataset: DD
      feature_name:
        - multi_1.2.3.4.5.6.7.8
      hidden_dim: [32, 64]
      signature: [rpf, rpf_lbl]
      num_epochs: 200
  proteins:
    default: false
    variables:
      dataset: PROTEINS
      feature_name:
        - freq_multi_1
        - freq_multi_1.2.3.4.5.6.7.8
        - freq_multi_1.2.3.4
        - freq_multi_1.3.5.7
      num_encoder_layers: 2
      num_decoder_layers: 4
      hidden_dim: [64, 128]
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      batch_size: 256
      num_epochs: 200
      multi_encoder: true
  enzymes:
    default: false
    variables:
      dataset: [ENZYMES]
      feature_name:
        - multi_1.2.3.4.5.6.7.8
      hidden_dim: [32, 64]
      batch_size: 768
      signature: [rpf, rpf_lbl]
      num_epochs: 200
  ptc:
    default: false
    variables:
      dataset:
        - PTC_MR
        - PTC_FR
        - PTC_MM
        - PTC_FM
      feature_name:
        #- multi_1
        - multi_1.2.3.4.5.6.7.8
      hidden_dim: [32, 64]
      batch_size: 512
      dropout: [0.0, 0.5]
      batch_norm: true
      signature: [rpf, rpf_lbl]
      learning_rate: 0.01
      num_encoder_layers: 2
      num_decoder_layers: 2
      num_epochs: 200
  reddit:
    default: false
    variables:
      dataset:
        - REDDIT-BINARY
      feature_name:
        #- freq_multi_1
        - freq_multi_1.2.3.4
        #- freq_multi_1.3.5.7
        #- freq_multi_1.2.3.4.5.6.7.8
        #- embed_rpf_2.4.6.8
        #- embed_rpf_4.8.12.16
        #- embed_rpf_2.4.6
        #- embed_rpf_3.6.9
      encoder_dim: 64, 128
      decoder_dim: 128, 64
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 64
      multi_encoder: true
      combine_encoder: weight_pooling
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit5k:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        - freq_multi_1.2.3.4
        - freq_multi_1.2.3.4.5.6.7.8
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      encoder_dim:
        - 64, 64
        - 64, 64, 64
        #- 64
      shared_encoder_dim:
      decoder_dim: 64, 64, 6
      dropout:
        - 0.0
        - 0.05
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 64
      multi_encoder: true
      combine_encoder: weight_pooling
      quantize_type: none
      append_embedding_to_encoder_output: true
      # batch_norm: true
  reddit5k_1:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        #- freq_multi_1.2.3.4
        - freq_multi_1.2.3.4.5.6.7.8
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      num_encoder_layers: 2
      num_decoder_layers: 4
      encoder_dim: 128
      decoder_dim: 128
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 50
      multi_encoder: true
      combine_encoder: weight_pooling
      quantize_type: none
      append_embedding_to_encoder_output: false
  reddit5k_2:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        - freq_multi_1.2.3.4
        #- freq_multi_1.2.3.4
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      encoder_dim: 128, 128
      decoder_dim: 128, 128, 128
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 64
      multi_encoder: true
      combine_encoder: weight_pooling_2d
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit5k_attn:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        - freq_multi_1.2.3.4
        #- freq_multi_1.2.3.4
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      encoder_dim: 128, 128
      decoder_dim: 128, 128, 128
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 64
      multi_encoder: true
      combine_encoder: attention
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit5k_3:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        #- freq_multi_1.2.3.4
        - freq_multi_1.2.3.4
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      num_encoder_layers: 2
      num_decoder_layers: 4
      encoder_dim: 128
      decoder_dim: 128
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 64
      multi_encoder: false
      combine_encoder: weight_pooling
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit5k_4:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        #- freq_multi_1.2.3.4
        - freq_multi_1.2.3.4
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      num_encoder_layers: 2
      num_decoder_layers: 4
      encoder_dim: 32
      decoder_dim: 128
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 64
      multi_encoder: true
      combine_encoder: concat
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit5k_5:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        #- freq_multi_1.2.3.4
        - freq_multi_1.2.3.4
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      num_encoder_layers: 2
      num_decoder_layers: 4
      encoder_dim: 32
      decoder_dim: 128
      dropout: 0.5
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 64
      multi_encoder: true
      combine_encoder: pooling
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit5k_6:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        #- freq_multi_1.2.3.4
        - freq_multi_1.5.9.13
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      num_encoder_layers: 2
      num_decoder_layers: 4
      encoder_dim: 32
      decoder_dim: 128
      dropout: 0.5
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 64
      multi_encoder: true
      combine_encoder: concat
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit5k_7:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        #- freq_multi_1.2.3.4
        - freq_multi_1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      encoder_dim: 64, 128
      decoder_dim: 128, 64, 32, 16
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 32
      multi_encoder: true
      combine_encoder: weight_pooling
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit5k_8:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        #- freq_multi_1.2.3.4
        - freq_multi_1.2.3.4.5.6.7.8
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      encoder_dim: 64, 128
      decoder_dim: 128, 64, 32, 16
      dropout: 0.1
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 64
      multi_encoder: true
      combine_encoder: weight_pooling
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit5k_9:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        - freq_multi_1.2.3.4
        #- freq_multi_1.2.3.4.5.6.7.8
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      encoder_dim: 16, 32, 64, 128
      decoder_dim: 128, 64, 32, 16
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 128
      multi_encoder: true
      combine_encoder: weight_pooling
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit5k_10:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        - freq_multi_1.2.3.4
        #- freq_multi_1.2.3.4.5.6.7.8
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      encoder_dim: 32, 64, 128, 256
      decoder_dim: 256, 128, 64, 32
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 128
      multi_encoder: false
      combine_encoder: weight_pooling
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit12k:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-12K
      feature_name:
        #- freq_multi_1
        #- freq_multi_1.2.3.4.5.6.7.8
        - freq_multi_1.2.3.4
        #- freq_multi_1.3.5.7
      encoder_dim: 128, 128
      decoder_dim: 128, 128, 128
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 64
      multi_encoder: true
      combine_encoder: weight_pooling
      quantize_type: none
      append_embedding_to_encoder_output: true
  reddit5k_11:
    default: false
    variables:
      dataset:
        - REDDIT-MULTI-5K
      feature_name:
        #- freq_multi_1
        - freq_multi_1.2.3.4
        #- freq_multi_1.2.3.4.5.6.7.8
        #- freq_multi_1.3.5.7
        #- freq_multi_1.3.5.7.9.11.13.15
      encoder_dim: 64, 128
      decoder_dim: 128, 64, 32, 16
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      num_epochs: 120
      batch_size: 256
      multi_encoder: false
      combine_encoder: weight_pooling
      quantize_type: none
      append_embedding_to_encoder_output: true
  imdb:
    default: false
    variables:
      dataset:
        - IMDB-BINARY
        - IMDB-MULTI
      feature_name:
        - freq_multi_1
        - freq_multi_1.2.3.4
        - freq_multi_1.2.3.4.5.6.7.8
      num_encoder_layers: 2
      num_decoder_layers: 4
      hidden_dim: [64, 128]
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      batch_size: 256
      num_epochs: 200
      multi_encoder: true
  collab:
    default: false
    variables:
      dataset:
        - COLLAB
      feature_name:
        #- multi_1.2.3.4.5.6.7.8
        - freq_multi_1
        #- freq_multi_1.2.3.4
        - freq_multi_1.2.3.4.5.6.7.8
      num_encoder_layers: 2
      num_decoder_layers: 4
      hidden_dim: [128]
      dropout: 0.2
      signature: rpf
      learning_rate: 0.01
      batch_size: 128
      num_epochs: 200
      multi_encoder: true
  cox2_dhfr:
    default: false
    variables:
      dataset: [COX2, DHFR]
      feature_name:
        - multi_1.2.3.4.5.6.7.8
      signature: [rpf, rpf_lbl]
      hidden_dim: 32
      num_epochs: 500
      batch_size: 728
      batch_norm: true
      num_encoder_layers: 2
      num_decoder_layers: 2
      # dropout: 0.2
  frank:
    default: false
    variables:
      dataset:
        - FRANKENSTEIN
      feature_name:
        - embed_rpf_1
        - embed_rpf_2.4.6.8
        - embed_rpf_1.3.5.7
        - embed_rpf_1.2.3.4.5.6.7.8
      hidden_dim: 32
      dense_dim: 32
      signature: [rpf]
    report:
      type: raw
      reduce: []
  synthetic:
    default: false
    variables:
      dataset:
        - SYNTHETIC
      feature_name:
        - embed_rpf_1
        - embed_rpf_2.4.6.8
        - embed_rpf_1.3.5.7
        - embed_rpf_1.2.3.4.5.6.7.8
      hidden_dim: 64
      dense_dim: 64
      signature: [rpf, rpf_lbl]
model:
  name: src.models.set_transformer.MultiWeightedDeepSets
  attn_num_heads: 1
  attn_num_layers: 2
  encoder_dim: ~encoder_dim
  shared_encoder_dim: ~shared_encoder_dim
  decoder_dim: ~decoder_dim
  dense_dim: ~hidden_dim
  dropout: ~dropout
  batch_norm: false
  multi_encoder: ~multi_encoder
  combine_encoder: ~combine_encoder
  relu_before_pooling: false
  weight_activation: softmax
  append_embedding_to_encoder_output: ~append_embedding_to_encoder_output
dataset:
  name: src.datasets.grakel.Grakel
  valid_set_ratio: 0.1
  dataset_name: ~dataset
  shuffle: true
  graph_features:
    persistence_diagram:
      type: persistence_diagram
      key: ~feature_name
      filtration: vertex_weight
      signature: ~signature
      max_length: 10000
      quantize:
        type: ~quantize_type
        size: ~quantize_size
  reuse: true
  test_size: 0.1
  output_format: none
train:
  valid_set: valid
  num_epochs: ~num_epochs
  # early_stop:
  #   num_epochs: 50
  batch_size: ~batch_size
  optimizer:
    name: adam
    lr: ~learning_rate
  lr_scheduler:
    # milestones: [15, 30, 45, 60]
    # milestones: [25, 50, 75, 100, 125, 150]
    milestones: [20, 40, 60, 80]
    gamma: 0.2
  cross_validation: 10
  select_model: best
  save_every: 1000e  # never save
  log_every:
  show_report: true
  show_progress: true
test:
  test_sets: [test]
  metrics: [acc]